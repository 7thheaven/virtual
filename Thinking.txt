20140417

完成XenServer下的移植。需要明确下一步的目标。

物理机的故障注入，一般是对cpu、内存、磁盘、寄存器等硬件进行，
可以通过软件模拟达到硬件故障的效果，方式多样：
面向函数的系统级方式、使用API的用户级方式、汇编直接访问硬件等。

在虚拟机上同样可以进行物理机的那些故障注入，
但是作为一个云评测工具，应该面向的是整个的云环境，
在进行单机故障注入的时候可以且只可以评测对故障的隔离性，
进行环境层次的故障注入更有意义，
比如同一宿主物理机下的不同虚拟机之间的非法内存访问（可以通过VMM故障实现），
更应该考验的是作为云环境核心的虚拟化管理能力，也就是作为Hypervisor的VMM，

在VMM的宿主机上进行传统的物理机故障测试？可以考察单点故障问题。

对VMM的虚拟化管理功能进行故障测试？

破坏原有功能是否有意义？只是单纯地进行破坏可能不会有意义，进行有目的性的破坏，比如VM页表篡改，可能导致VM之间非法访问内存，这类是有意义的。

非破坏性的测试？检测调用接口的健壮性（如参数合法性判断）？生成大量随机参数进行测试？
结果很可能会取决于调用的层次，

如果目标是底层调用，即通常只会被系统自身代码调用的接口，很可能会发现健壮性很差，因为不会被随意调用，系统自身使用时往往是合理的，不需要判断参数（或者说上层已经判断过了，保证在调用底层函数时参数是合法的）。

我也是通过这个特性（底层调用未对参数合理性进行判断，或者难以进行判断，如页表的地址等运行时才知道是否合理的信息），来进行一些故障实现，如篡改VM页表信息。

如果目标换成上层调用，由于是可能常被用户使用的调用，所以通常系统会对参数的合法性进行判断，也就是有一定的健壮性。
但是对这些参数健壮性的测试，可以通过阅读调用的源代码来完全预测出结果，这样是否有意义？
或者说这样的测试只对非开源系统才有意义吗？

是否还有其他的测试对象呢？排除性能等其他测试方向，仅从容错、可靠性的目标来说。暂时没想到。
或者说，是否一个系统所有的功能都是通过函数来实现的呢？
面向设备的（物理机方式故障）、面向函数调用的（系统功能故障），
是否还有其他的故障/评测目标？

暂时还没想到，如果没有的话，那么可以说只要处理了面向函数的故障即可完成所有的功能可靠性评测，
目前通过拦截/探针的方式进行，那么没有导出到符号表的函数例程怎么测试？
或者说，是否有其他方式测试函数调用？
对于系统层来说，重要的功能函数调用都存在于或者间接存在于符号表了吧？我想是的。
那么，接下来的任务可能是搞清符号表中的函数例程各自的作用？
或者是逆向考虑，对每个需要测试的功能，找到其与符号表中核心函数的关系？
也就是说是对操作系统的各调用的实现进行学习了。
若如此，为了开发一个不同系统下通用的评测工具，可能需要对类linux的系统的通用的系统功能函数做一个筛选统计，
或者直奔主题，仅针对vmm的功能函数做研究，研究一下开源的xen、kvm，非开源的vmware暂时不考虑。


20140423

上周开会中与老师讨论，说可以尝试在VMM宿主机上进行物理机的故障注入测试，然后考察硬件资源故障对虚拟化资源分配会产生什么影响，正好可以用到实验室前几辈学长留下来的物理机故障注入工具。
于是看了下之前的寄存器故障注入工具，测试的原理是在触发中断的时候，系统会保存寄存器现场到内存中，该工具修改了内存中相应的值，在系统恢复的时候，会把该值写到寄存器中。
也就是说，实际上，这个测试技术针对的层面是进程级，并不是系统级，而且是利用内存故障的原理。
这样看，似乎和我要做的东西有些偏差。
应该实现的是更底层的系统级的寄存器故障。
而且该工具其实只是用了kprobes技术对do_timer和force_sig_info进行了修改，其中对do_timer的监听是为了周期性地触发自己的处理函数，该函数扫描系统当前所有进程，找到指定PID的进程修改其寄存器值。

如果说所有的进程（init除外）都是do_fork产生的话，是否可以通过拦截do_fork实现对所有进程的寄存器修改，来模拟硬件级的寄存器故障呢？
接下来可能去试试这个方法。

20140425

初步实现了一个基于对do_fork拦截的寄存器故障注入工具，并做了简单的测试，修改ax、bx、cx等寄存器未发现系统异常，修改sp会导致系统崩溃并无法自动修复。

这个版本的工具隐含一个问题――拦截do_fork只能对开始注入之后的进程有效，对之前就存在于系统中的进程无效，可能还需要配合do_timer主动扫描进程?


开会后和老师讨论了一下，也验证了不少自己的想法。
对于用软件模拟实现硬件故障，很多部分是做不了的。

将计算机大致分为CPU、内存、磁盘、输入输出设备这几个部分：

其中我之前一直在做的是内存；

对于CPU，我想存储器和控制器用软件模拟故障应该是做不到的，

因为操作系统是连接硬件和软件的途径，若操作系统只是通过机器指令01序列使用该部分硬件的功能，那么在操作系统和操作系统之上的层次都是无法针对该硬件进行故障注入的，只能通过物理手段（电磁干扰之类的），

而我们之所以能够对CPU之中的寄存器进行故障注入，是因为该故障注入的手段本质上只是正常使用了寄存器硬件的功能――读和写，只不过是我们修改了其读写的值而已，从硬件上看其实并没有什么故障，只是我们通过这种方式能够模拟出寄存器存取不准了（比如硬件上有一个01-Bit坏掉了），效果是一样的。

也就是说，我们对这些底层硬件，只能通过正常使用它们的功能来注入故障，即从更高层的逻辑去看去做这个事。

那么，对于运算器、控制器，可以说寄存器是它们与外界的接口，通过寄存器故障即可模拟它们的故障，
比如
将存放运算器结果的寄存器值改变 就相当于 运算器算错了 的效果；
将存放下一条指令地址的寄存器值改变 就相当于 控制器出错了 的效果。

对于磁盘，故障的模拟应该很简单，比如改写一个文件，或者拦截文件相关的系统调用read、write、open等等，但是由于磁盘故障引起的后果往往是通过重启无法修复的，这与只存在于内存中的故障不同，所以故障可能会导致整个系统环境的完全崩溃且无法修复（内存故障只需要重启修复即可），这样每次严重故障之后就需要重新搭建整个实验环境，从可行性的角度来看，代价太大，所以可以先不考虑这个方向的工作。

对于输入输出设备，可以以驱动为切入点注入故障，应该也不难，但是这和我本来云计算、虚拟化相关的方向联系不大，所以也先不考虑。

综上所述，工作的重心还是在CPU和内存上，内存之前已经做了不少了，CPU可以通过寄存器手段模拟其他部件故障，所以接下来还是继续研究寄存器的故障注入。


20140507

向系统注入了所有进程所有寄存器永久一位翻转的故障，系统崩溃了，而且冷关机重新启动也不能修复这个问题。

实验环境可能坏掉了，惨了惨了。
接下来可能是头疼的重新搭环境吧，闹心。

20140618

配置环境的网络很重要，简单记录一下以备忘。
子网掩码255.255.0.0，网关192.168.0.1，DNS 202.118.224.101和202.118.224.100，时间用ntp pool.ntp.org。
设置ddk的network为network0，可以得到ipv4的地址，然后用xftp即可传输文件进行交叉编译开发。

20141031

关于测试的自动化平台相关。

散式任务还要完成：
设计实现负载。
设计实现监测。
其他待细化。

自动化平台方面主要有几种方式可选：
1.――python ssh――
可以。
设计实现UI。
完善细化，可以做成一个可扩展的平台，故障模型以xml之类的配置文件形式描述，然后测试的命令流也设计成可描述的文件形式，这样平台的可拓展性很好，不会python的人也可以用。	退一步讲，可以用python的frame扩展，不过需要接入方自行写python。	有空先尝试第一种较好的方式。

服务器模式？后备。	python ssh应该就足够解决问题且不需要对待测机进行安装通信客户端，CS模式还需要去待测机一一安装通信用的客户端。

STAF？貌似只是通信机制，监控负载以及UI要自己做，估计不如python＋ssh的方式，但是实验室之前的工具都用了staf通信，已经整合在一起，要新加的话追求速度还是要用这个快，都改成python＋ssh需要阅读源码进行修改，所以导师目前要用这个实现方式。	传统的工具用这个应该可以，容错性健壮性测试应该开发新型的平台，因为监控和负载都需要特殊定制。

综上，先把CPU寄存器相关的故障测试加到STAF里，虚拟化方面的测试用python＋ssh写，争取留下一个可扩展平台。

20150303

平台相关基本完毕。

按实验室要求，平台对用户易用化的优先要高于可扩展性，所以对平台的扩展需要写新的python frame加进来。

日志的话，可以通过scp传递、也可以在ssh中远程输出到std流中然后通过ssh返回的stdout获取。
目前还没有处理系统崩溃、无响应的时候的日志回收。

负载方面，其实内存上的系统级故障，注入后很快就会达到目标次数，不用负载也可触发，但是可能不会捕捉到一些进程级的故障结果。
难以触发的故障需要设计负载，例如很多超级调用。
